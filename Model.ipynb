{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Sum_greater_than_0.5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js1CK4Gtv-_f",
        "colab_type": "text"
      },
      "source": [
        "# **USING NEURAL NETWORK FOR COMPARISON**\n",
        "## Our goal is to train a neural network which calculate the sum of given (say 4) numbers and then compare wether the result is less than a number (say 0.5) or not.\n",
        "(I am using Google Colaboratory GPU for this.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTuWD1Q6xsiR",
        "colab_type": "text"
      },
      "source": [
        "# To run on GPU, run the next cell\n",
        "(Make sure to change runtime as GPU, if you're using Google Colaboratory.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTWfDnvdtviL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d29245d1-15a6-4bec-d7b9-581ed000c7de"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "import keras.backend.tensorflow_backend as tfback\n",
        "\n",
        "def _get_available_gpus():\n",
        "    \n",
        "    #global _LOCAL_DEVICES\n",
        "    if tfback._LOCAL_DEVICES is None:\n",
        "        devices = tf.config.list_logical_devices()\n",
        "        tfback._LOCAL_DEVICES = [x.name for x in devices]\n",
        "    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n",
        "\n",
        "tfback._get_available_gpus = _get_available_gpus\n",
        "\n",
        "\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQGdpuOkL2H_",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive for saving our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rwiORkhLa-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12606d97-856a-4f30-f00f-3b40099ef3d8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0h6BI10C_kU",
        "colab_type": "text"
      },
      "source": [
        "# Load important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajuhcpABpr7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV5lyS0UDSOO",
        "colab_type": "text"
      },
      "source": [
        "# Create Dataset\n",
        "We will create a numpy array having four values for each example such that the sum of values get distributed almost equally about 0.5\n",
        "\n",
        "Our output will be either 0 (for sum < 0.5) or 1 (for sum >= 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq1XPNLdpr8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def actual_output(X):\n",
        "    # Comparing X with 0.5 to get output\n",
        "    Y = (np.sum(X, axis = 1) >= 0.5)*1\n",
        "    \n",
        "    return Y"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_RqKCPpr8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset():\n",
        "    # Create random float values for input. Since we will sum 4 input values, so division by 4 is done to keep the sum between 0 and 1\n",
        "    X_train = np.random.rand(100000, 4)/4\n",
        "    X_val = np.random.rand(20000, 4)/4\n",
        "    X_test = np.random.rand(10000, 4)/4\n",
        "    # Take output for corresponding X\n",
        "    Y_train = actual_output(X_train)\n",
        "    Y_val = actual_output(X_val)\n",
        "    Y_test = actual_output(X_test)\n",
        "    \n",
        "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaEmQkkXpr8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "7cdc8eb9-94be-41b8-e4a2-af9519222a87"
      },
      "source": [
        "X_train, Y_train, _, _, _, _ = create_dataset()\n",
        "print('Shape of X_train : ' + str(X_train.shape))\n",
        "print('Shape of Y_train : ' + str(Y_train.shape) + '\\n')\n",
        "print('X_train : ')\n",
        "print(X_train)\n",
        "print('\\nY_train : ')\n",
        "print(Y_train)\n",
        "print('\\nTotal count of ones in Y_train: ' + str(np.count_nonzero(Y_train == 1)))\n",
        "del X_train, Y_train"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X_train : (100000, 4)\n",
            "Shape of Y_train : (100000,)\n",
            "\n",
            "X_train : \n",
            "[[0.13174169 0.08742294 0.14872284 0.14351316]\n",
            " [0.16043738 0.18175475 0.06939325 0.12509022]\n",
            " [0.01782679 0.02797315 0.07812498 0.17603068]\n",
            " ...\n",
            " [0.09703257 0.10298075 0.17018159 0.13408452]\n",
            " [0.17267154 0.11877749 0.24565443 0.19629387]\n",
            " [0.12234273 0.12247236 0.04911706 0.21328029]]\n",
            "\n",
            "Y_train : \n",
            "[1 1 0 ... 1 1 1]\n",
            "\n",
            "Total count of ones in Y_train: 49755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gKlF75_MPaA",
        "colab_type": "text"
      },
      "source": [
        "We couls see that approximately half of the values are one and half are zeros. So, we will not have any issues of skew dataset.\n",
        "\n",
        "Note : Use np.random.rand (this is uniform distribution), not np.random.randn (this is normal distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa7dT_PLFnut",
        "colab_type": "text"
      },
      "source": [
        "# Create model\n",
        "We are doing a simple operation, and thus it can be achieved by just one Dense layer of neural network. In this one layer, the linear function will find the sum and the activation function will compare the sum with 0.5\n",
        "\n",
        "For the comparison, we could use sigmoid function.\n",
        "\n",
        "Mathematically, you could interpret that for the linear function, the weights should be in the ratio of [1, 1, 1, 1] and bias should be in the same ratio of [-0.5] ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaXIUqSqKokB",
        "colab_type": "text"
      },
      "source": [
        "![](https://miro.medium.com/max/2560/1*yIPIuNIn6ar7MvQnNqlWlQ.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZOiu60tpr8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_model():\n",
        "    # Create one Dense layer Sequential model\n",
        "    model = Sequential()\n",
        "    model.build(input_shape = (None, 4))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    # Compile the model\n",
        "    opt = SGD(learning_rate = 0.01, momentum = 0.99)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rer_OD0Epr8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7c4a3efa-7e16-4109-b360-debd5b2f277c"
      },
      "source": [
        "model = define_model()\n",
        "model.summary()\n",
        "del model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR6RxcFgLQiP",
        "colab_type": "text"
      },
      "source": [
        "### This performance function is to plot the Cross Entropy Loss and Classification Accuracy graphs at the end of training of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXttyShkpr8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance(histories):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    # Plot Loss\n",
        "    plt.subplot(2,2,1)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(histories.history['loss'], color = 'blue', label = 'train')\n",
        "    plt.plot(histories.history['val_loss'], color = 'red', label = 'test')\n",
        "    plt.legend(bbox_to_anchor=(0., 0., 0.2, 0.), loc='lower left', ncol=1, mode=\"expand\", borderaxespad=1.)\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(2,2,2)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(histories.history['accuracy'], color = 'blue', label = 'train')\n",
        "    plt.plot(histories.history['val_accuracy'], color = 'red', label = 'test')\n",
        "    plt.legend(bbox_to_anchor=(0., 0., 0.2, 0.), loc='lower left', ncol=1, mode=\"expand\", borderaxespad=1.)\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwoGQw6HLqyh",
        "colab_type": "text"
      },
      "source": [
        "# Training the model\n",
        "Now the training part. Give all the parameters to train our model efficiently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZUFS-atpr85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model():\n",
        "    # Load dataset\n",
        "    X_train, Y_train, X_val, Y_val, _, _ = create_dataset()\n",
        "    # Load model\n",
        "    model = define_model()\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, Y_train, epochs = 200, batch_size = 32, verbose = 1, validation_data = (X_val, Y_val))\n",
        "    # Plot performance graphs\n",
        "    performance(history)\n",
        "    # Evaluate accuracy on validation set\n",
        "    _, acc = model.evaluate(X_val, Y_val, verbose = 1)\n",
        "    print('\\nAccuracy of our Model on Validation Set: %.5f' % (acc*100))\n",
        "    # Save our model\n",
        "    model.save('/content/drive/My Drive/Rahul/ML/Is_sum_greater_than_0.5/One_layer_dense_model.h5')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm4MXDpLpr9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84f2f5eb-a749-4f92-b8c0-76badb94b390"
      },
      "source": [
        "train_model()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 20000 samples\n",
            "Epoch 1/200\n",
            "100000/100000 [==============================] - 8s 83us/step - loss: 0.3482 - accuracy: 0.9515 - val_loss: 0.2319 - val_accuracy: 0.9948\n",
            "Epoch 2/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.2000 - accuracy: 0.9918 - val_loss: 0.1817 - val_accuracy: 0.9688\n",
            "Epoch 3/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.1642 - accuracy: 0.9917 - val_loss: 0.1536 - val_accuracy: 0.9919\n",
            "Epoch 4/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.1450 - accuracy: 0.9931 - val_loss: 0.1385 - val_accuracy: 0.9905\n",
            "Epoch 5/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.1322 - accuracy: 0.9936 - val_loss: 0.1287 - val_accuracy: 0.9862\n",
            "Epoch 6/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.1229 - accuracy: 0.9957 - val_loss: 0.1197 - val_accuracy: 0.9914\n",
            "Epoch 7/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.1158 - accuracy: 0.9958 - val_loss: 0.1140 - val_accuracy: 0.9885\n",
            "Epoch 8/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.1101 - accuracy: 0.9950 - val_loss: 0.1078 - val_accuracy: 0.9974\n",
            "Epoch 9/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.1053 - accuracy: 0.9955 - val_loss: 0.1036 - val_accuracy: 0.9948\n",
            "Epoch 10/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.1013 - accuracy: 0.9954 - val_loss: 0.0996 - val_accuracy: 0.9994\n",
            "Epoch 11/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0978 - accuracy: 0.9947 - val_loss: 0.0964 - val_accuracy: 0.9971\n",
            "Epoch 12/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0948 - accuracy: 0.9961 - val_loss: 0.0934 - val_accuracy: 0.9995\n",
            "Epoch 13/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0920 - accuracy: 0.9963 - val_loss: 0.0910 - val_accuracy: 0.9962\n",
            "Epoch 14/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0896 - accuracy: 0.9963 - val_loss: 0.0891 - val_accuracy: 0.9934\n",
            "Epoch 15/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0874 - accuracy: 0.9959 - val_loss: 0.0866 - val_accuracy: 0.9965\n",
            "Epoch 16/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0854 - accuracy: 0.9963 - val_loss: 0.0859 - val_accuracy: 0.9883\n",
            "Epoch 17/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0835 - accuracy: 0.9972 - val_loss: 0.0830 - val_accuracy: 0.9931\n",
            "Epoch 18/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0820 - accuracy: 0.9965 - val_loss: 0.0811 - val_accuracy: 0.9989\n",
            "Epoch 19/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0803 - accuracy: 0.9970 - val_loss: 0.0795 - val_accuracy: 0.9983\n",
            "Epoch 20/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0788 - accuracy: 0.9975 - val_loss: 0.0781 - val_accuracy: 0.9995\n",
            "Epoch 21/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0775 - accuracy: 0.9965 - val_loss: 0.0774 - val_accuracy: 0.9940\n",
            "Epoch 22/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0763 - accuracy: 0.9973 - val_loss: 0.0759 - val_accuracy: 0.9944\n",
            "Epoch 23/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0752 - accuracy: 0.9965 - val_loss: 0.0758 - val_accuracy: 0.9900\n",
            "Epoch 24/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0740 - accuracy: 0.9973 - val_loss: 0.0733 - val_accuracy: 0.9990\n",
            "Epoch 25/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0730 - accuracy: 0.9973 - val_loss: 0.0723 - val_accuracy: 0.9994\n",
            "Epoch 26/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0720 - accuracy: 0.9966 - val_loss: 0.0714 - val_accuracy: 0.9990\n",
            "Epoch 27/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0710 - accuracy: 0.9974 - val_loss: 0.0704 - val_accuracy: 0.9994\n",
            "Epoch 28/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0701 - accuracy: 0.9973 - val_loss: 0.0696 - val_accuracy: 0.9980\n",
            "Epoch 29/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0693 - accuracy: 0.9973 - val_loss: 0.0687 - val_accuracy: 0.9977\n",
            "Epoch 30/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0684 - accuracy: 0.9970 - val_loss: 0.0682 - val_accuracy: 0.9962\n",
            "Epoch 31/200\n",
            "100000/100000 [==============================] - 7s 67us/step - loss: 0.0677 - accuracy: 0.9976 - val_loss: 0.0673 - val_accuracy: 0.9973\n",
            "Epoch 32/200\n",
            "100000/100000 [==============================] - 7s 72us/step - loss: 0.0670 - accuracy: 0.9969 - val_loss: 0.0664 - val_accuracy: 0.9995\n",
            "Epoch 33/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0662 - accuracy: 0.9967 - val_loss: 0.0658 - val_accuracy: 0.9991\n",
            "Epoch 34/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0655 - accuracy: 0.9970 - val_loss: 0.0651 - val_accuracy: 0.9995\n",
            "Epoch 35/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0649 - accuracy: 0.9976 - val_loss: 0.0644 - val_accuracy: 0.9984\n",
            "Epoch 36/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0642 - accuracy: 0.9974 - val_loss: 0.0641 - val_accuracy: 0.9962\n",
            "Epoch 37/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0637 - accuracy: 0.9973 - val_loss: 0.0633 - val_accuracy: 0.9962\n",
            "Epoch 38/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0631 - accuracy: 0.9966 - val_loss: 0.0626 - val_accuracy: 0.9995\n",
            "Epoch 39/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0625 - accuracy: 0.9978 - val_loss: 0.0629 - val_accuracy: 0.9933\n",
            "Epoch 40/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0619 - accuracy: 0.9975 - val_loss: 0.0629 - val_accuracy: 0.9909\n",
            "Epoch 41/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0614 - accuracy: 0.9975 - val_loss: 0.0613 - val_accuracy: 0.9964\n",
            "Epoch 42/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0610 - accuracy: 0.9967 - val_loss: 0.0609 - val_accuracy: 0.9955\n",
            "Epoch 43/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0605 - accuracy: 0.9973 - val_loss: 0.0600 - val_accuracy: 0.9992\n",
            "Epoch 44/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0600 - accuracy: 0.9973 - val_loss: 0.0604 - val_accuracy: 0.9934\n",
            "Epoch 45/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0596 - accuracy: 0.9966 - val_loss: 0.0591 - val_accuracy: 0.9994\n",
            "Epoch 46/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0590 - accuracy: 0.9978 - val_loss: 0.0587 - val_accuracy: 0.9965\n",
            "Epoch 47/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0586 - accuracy: 0.9975 - val_loss: 0.0582 - val_accuracy: 0.9995\n",
            "Epoch 48/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0582 - accuracy: 0.9976 - val_loss: 0.0579 - val_accuracy: 0.9976\n",
            "Epoch 49/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0578 - accuracy: 0.9973 - val_loss: 0.0574 - val_accuracy: 0.9987\n",
            "Epoch 50/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0573 - accuracy: 0.9979 - val_loss: 0.0572 - val_accuracy: 0.9971\n",
            "Epoch 51/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0570 - accuracy: 0.9978 - val_loss: 0.0566 - val_accuracy: 0.9995\n",
            "Epoch 52/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0566 - accuracy: 0.9982 - val_loss: 0.0563 - val_accuracy: 0.9977\n",
            "Epoch 53/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0562 - accuracy: 0.9975 - val_loss: 0.0563 - val_accuracy: 0.9955\n",
            "Epoch 54/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0559 - accuracy: 0.9975 - val_loss: 0.0555 - val_accuracy: 0.9994\n",
            "Epoch 55/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0555 - accuracy: 0.9979 - val_loss: 0.0562 - val_accuracy: 0.9927\n",
            "Epoch 56/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0552 - accuracy: 0.9979 - val_loss: 0.0549 - val_accuracy: 0.9979\n",
            "Epoch 57/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0549 - accuracy: 0.9973 - val_loss: 0.0547 - val_accuracy: 0.9966\n",
            "Epoch 58/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0545 - accuracy: 0.9977 - val_loss: 0.0541 - val_accuracy: 0.9980\n",
            "Epoch 59/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0542 - accuracy: 0.9977 - val_loss: 0.0543 - val_accuracy: 0.9954\n",
            "Epoch 60/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0539 - accuracy: 0.9977 - val_loss: 0.0536 - val_accuracy: 0.9981\n",
            "Epoch 61/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0536 - accuracy: 0.9978 - val_loss: 0.0533 - val_accuracy: 0.9973\n",
            "Epoch 62/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0533 - accuracy: 0.9976 - val_loss: 0.0529 - val_accuracy: 0.9981\n",
            "Epoch 63/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0530 - accuracy: 0.9974 - val_loss: 0.0527 - val_accuracy: 0.9972\n",
            "Epoch 64/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0527 - accuracy: 0.9976 - val_loss: 0.0524 - val_accuracy: 0.9989\n",
            "Epoch 65/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0525 - accuracy: 0.9975 - val_loss: 0.0522 - val_accuracy: 0.9974\n",
            "Epoch 66/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0522 - accuracy: 0.9979 - val_loss: 0.0519 - val_accuracy: 0.9981\n",
            "Epoch 67/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0519 - accuracy: 0.9982 - val_loss: 0.0516 - val_accuracy: 0.9986\n",
            "Epoch 68/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0516 - accuracy: 0.9975 - val_loss: 0.0513 - val_accuracy: 0.9995\n",
            "Epoch 69/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0514 - accuracy: 0.9975 - val_loss: 0.0512 - val_accuracy: 0.9977\n",
            "Epoch 70/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0511 - accuracy: 0.9982 - val_loss: 0.0508 - val_accuracy: 0.9990\n",
            "Epoch 71/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0509 - accuracy: 0.9981 - val_loss: 0.0505 - val_accuracy: 0.9995\n",
            "Epoch 72/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0506 - accuracy: 0.9980 - val_loss: 0.0503 - val_accuracy: 0.9988\n",
            "Epoch 73/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0504 - accuracy: 0.9979 - val_loss: 0.0500 - val_accuracy: 0.9995\n",
            "Epoch 74/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0502 - accuracy: 0.9980 - val_loss: 0.0498 - val_accuracy: 0.9993\n",
            "Epoch 75/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0499 - accuracy: 0.9981 - val_loss: 0.0503 - val_accuracy: 0.9947\n",
            "Epoch 76/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0498 - accuracy: 0.9973 - val_loss: 0.0497 - val_accuracy: 0.9966\n",
            "Epoch 77/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0495 - accuracy: 0.9977 - val_loss: 0.0491 - val_accuracy: 0.9991\n",
            "Epoch 78/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0493 - accuracy: 0.9978 - val_loss: 0.0489 - val_accuracy: 0.9996\n",
            "Epoch 79/200\n",
            "100000/100000 [==============================] - 7s 70us/step - loss: 0.0491 - accuracy: 0.9984 - val_loss: 0.0488 - val_accuracy: 0.9985\n",
            "Epoch 80/200\n",
            "100000/100000 [==============================] - 7s 71us/step - loss: 0.0489 - accuracy: 0.9978 - val_loss: 0.0486 - val_accuracy: 0.9980\n",
            "Epoch 81/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0486 - accuracy: 0.9980 - val_loss: 0.0483 - val_accuracy: 0.9994\n",
            "Epoch 82/200\n",
            "100000/100000 [==============================] - 7s 68us/step - loss: 0.0484 - accuracy: 0.9984 - val_loss: 0.0481 - val_accuracy: 0.9984\n",
            "Epoch 83/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0483 - accuracy: 0.9979 - val_loss: 0.0481 - val_accuracy: 0.9976\n",
            "Epoch 84/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0481 - accuracy: 0.9975 - val_loss: 0.0478 - val_accuracy: 0.9977\n",
            "Epoch 85/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0479 - accuracy: 0.9979 - val_loss: 0.0475 - val_accuracy: 0.9994\n",
            "Epoch 86/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0477 - accuracy: 0.9982 - val_loss: 0.0475 - val_accuracy: 0.9973\n",
            "Epoch 87/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0475 - accuracy: 0.9979 - val_loss: 0.0473 - val_accuracy: 0.9977\n",
            "Epoch 88/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0473 - accuracy: 0.9980 - val_loss: 0.0471 - val_accuracy: 0.9979\n",
            "Epoch 89/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0471 - accuracy: 0.9981 - val_loss: 0.0469 - val_accuracy: 0.9981\n",
            "Epoch 90/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0470 - accuracy: 0.9981 - val_loss: 0.0466 - val_accuracy: 0.9983\n",
            "Epoch 91/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0468 - accuracy: 0.9974 - val_loss: 0.0465 - val_accuracy: 0.9969\n",
            "Epoch 92/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0466 - accuracy: 0.9983 - val_loss: 0.0464 - val_accuracy: 0.9977\n",
            "Epoch 93/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0464 - accuracy: 0.9984 - val_loss: 0.0460 - val_accuracy: 0.9990\n",
            "Epoch 94/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0463 - accuracy: 0.9977 - val_loss: 0.0460 - val_accuracy: 0.9979\n",
            "Epoch 95/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0461 - accuracy: 0.9978 - val_loss: 0.0464 - val_accuracy: 0.9948\n",
            "Epoch 96/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0459 - accuracy: 0.9981 - val_loss: 0.0458 - val_accuracy: 0.9973\n",
            "Epoch 97/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0458 - accuracy: 0.9978 - val_loss: 0.0455 - val_accuracy: 0.9980\n",
            "Epoch 98/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0456 - accuracy: 0.9984 - val_loss: 0.0453 - val_accuracy: 0.9995\n",
            "Epoch 99/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0455 - accuracy: 0.9981 - val_loss: 0.0457 - val_accuracy: 0.9954\n",
            "Epoch 100/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0453 - accuracy: 0.9978 - val_loss: 0.0452 - val_accuracy: 0.9969\n",
            "Epoch 101/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0452 - accuracy: 0.9979 - val_loss: 0.0454 - val_accuracy: 0.9950\n",
            "Epoch 102/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0450 - accuracy: 0.9982 - val_loss: 0.0446 - val_accuracy: 0.9995\n",
            "Epoch 103/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0448 - accuracy: 0.9983 - val_loss: 0.0445 - val_accuracy: 0.9995\n",
            "Epoch 104/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0447 - accuracy: 0.9984 - val_loss: 0.0451 - val_accuracy: 0.9948\n",
            "Epoch 105/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0446 - accuracy: 0.9981 - val_loss: 0.0443 - val_accuracy: 0.9984\n",
            "Epoch 106/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0444 - accuracy: 0.9982 - val_loss: 0.0441 - val_accuracy: 0.9973\n",
            "Epoch 107/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0443 - accuracy: 0.9981 - val_loss: 0.0439 - val_accuracy: 0.9990\n",
            "Epoch 108/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0442 - accuracy: 0.9979 - val_loss: 0.0446 - val_accuracy: 0.9944\n",
            "Epoch 109/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0440 - accuracy: 0.9981 - val_loss: 0.0437 - val_accuracy: 0.9981\n",
            "Epoch 110/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0439 - accuracy: 0.9983 - val_loss: 0.0435 - val_accuracy: 0.9984\n",
            "Epoch 111/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0437 - accuracy: 0.9981 - val_loss: 0.0437 - val_accuracy: 0.9966\n",
            "Epoch 112/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0436 - accuracy: 0.9985 - val_loss: 0.0434 - val_accuracy: 0.9977\n",
            "Epoch 113/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0435 - accuracy: 0.9981 - val_loss: 0.0432 - val_accuracy: 0.9979\n",
            "Epoch 114/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0433 - accuracy: 0.9982 - val_loss: 0.0435 - val_accuracy: 0.9956\n",
            "Epoch 115/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0432 - accuracy: 0.9982 - val_loss: 0.0429 - val_accuracy: 0.9990\n",
            "Epoch 116/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0431 - accuracy: 0.9983 - val_loss: 0.0430 - val_accuracy: 0.9967\n",
            "Epoch 117/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0429 - accuracy: 0.9981 - val_loss: 0.0429 - val_accuracy: 0.9967\n",
            "Epoch 118/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0428 - accuracy: 0.9978 - val_loss: 0.0424 - val_accuracy: 0.9985\n",
            "Epoch 119/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0427 - accuracy: 0.9984 - val_loss: 0.0424 - val_accuracy: 0.9987\n",
            "Epoch 120/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0426 - accuracy: 0.9984 - val_loss: 0.0422 - val_accuracy: 0.9994\n",
            "Epoch 121/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0424 - accuracy: 0.9980 - val_loss: 0.0421 - val_accuracy: 0.9995\n",
            "Epoch 122/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0424 - accuracy: 0.9983 - val_loss: 0.0420 - val_accuracy: 0.9989\n",
            "Epoch 123/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0422 - accuracy: 0.9980 - val_loss: 0.0418 - val_accuracy: 0.9985\n",
            "Epoch 124/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0421 - accuracy: 0.9985 - val_loss: 0.0417 - val_accuracy: 0.9991\n",
            "Epoch 125/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0421 - accuracy: 0.9979 - val_loss: 0.0422 - val_accuracy: 0.9955\n",
            "Epoch 126/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0419 - accuracy: 0.9979 - val_loss: 0.0418 - val_accuracy: 0.9967\n",
            "Epoch 127/200\n",
            "100000/100000 [==============================] - 7s 68us/step - loss: 0.0418 - accuracy: 0.9979 - val_loss: 0.0414 - val_accuracy: 0.9993\n",
            "Epoch 128/200\n",
            "100000/100000 [==============================] - 7s 71us/step - loss: 0.0416 - accuracy: 0.9982 - val_loss: 0.0413 - val_accuracy: 0.9994\n",
            "Epoch 129/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0415 - accuracy: 0.9983 - val_loss: 0.0412 - val_accuracy: 0.9995\n",
            "Epoch 130/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0414 - accuracy: 0.9984 - val_loss: 0.0412 - val_accuracy: 0.9984\n",
            "Epoch 131/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0414 - accuracy: 0.9979 - val_loss: 0.0411 - val_accuracy: 0.9982\n",
            "Epoch 132/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0412 - accuracy: 0.9986 - val_loss: 0.0408 - val_accuracy: 0.9992\n",
            "Epoch 133/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0411 - accuracy: 0.9983 - val_loss: 0.0408 - val_accuracy: 0.9984\n",
            "Epoch 134/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0410 - accuracy: 0.9980 - val_loss: 0.0413 - val_accuracy: 0.9955\n",
            "Epoch 135/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0409 - accuracy: 0.9986 - val_loss: 0.0405 - val_accuracy: 0.9994\n",
            "Epoch 136/200\n",
            "100000/100000 [==============================] - 7s 66us/step - loss: 0.0408 - accuracy: 0.9985 - val_loss: 0.0404 - val_accuracy: 0.9995\n",
            "Epoch 137/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0407 - accuracy: 0.9980 - val_loss: 0.0404 - val_accuracy: 0.9979\n",
            "Epoch 138/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0406 - accuracy: 0.9982 - val_loss: 0.0403 - val_accuracy: 0.9990\n",
            "Epoch 139/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0405 - accuracy: 0.9983 - val_loss: 0.0404 - val_accuracy: 0.9970\n",
            "Epoch 140/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0404 - accuracy: 0.9980 - val_loss: 0.0400 - val_accuracy: 0.9992\n",
            "Epoch 141/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0403 - accuracy: 0.9984 - val_loss: 0.0404 - val_accuracy: 0.9962\n",
            "Epoch 142/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0402 - accuracy: 0.9983 - val_loss: 0.0398 - val_accuracy: 0.9994\n",
            "Epoch 143/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0401 - accuracy: 0.9987 - val_loss: 0.0398 - val_accuracy: 0.9987\n",
            "Epoch 144/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0400 - accuracy: 0.9982 - val_loss: 0.0400 - val_accuracy: 0.9964\n",
            "Epoch 145/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0399 - accuracy: 0.9988 - val_loss: 0.0396 - val_accuracy: 0.9987\n",
            "Epoch 146/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0398 - accuracy: 0.9981 - val_loss: 0.0395 - val_accuracy: 0.9993\n",
            "Epoch 147/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0397 - accuracy: 0.9987 - val_loss: 0.0394 - val_accuracy: 0.9988\n",
            "Epoch 148/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0397 - accuracy: 0.9979 - val_loss: 0.0394 - val_accuracy: 0.9983\n",
            "Epoch 149/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0396 - accuracy: 0.9982 - val_loss: 0.0394 - val_accuracy: 0.9975\n",
            "Epoch 150/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0395 - accuracy: 0.9984 - val_loss: 0.0392 - val_accuracy: 0.9984\n",
            "Epoch 151/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0394 - accuracy: 0.9987 - val_loss: 0.0391 - val_accuracy: 0.9983\n",
            "Epoch 152/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0393 - accuracy: 0.9984 - val_loss: 0.0389 - val_accuracy: 0.9992\n",
            "Epoch 153/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0392 - accuracy: 0.9986 - val_loss: 0.0388 - val_accuracy: 0.9995\n",
            "Epoch 154/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0391 - accuracy: 0.9983 - val_loss: 0.0389 - val_accuracy: 0.9983\n",
            "Epoch 155/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0391 - accuracy: 0.9982 - val_loss: 0.0387 - val_accuracy: 0.9995\n",
            "Epoch 156/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0390 - accuracy: 0.9986 - val_loss: 0.0386 - val_accuracy: 0.9992\n",
            "Epoch 157/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0389 - accuracy: 0.9982 - val_loss: 0.0385 - val_accuracy: 0.9986\n",
            "Epoch 158/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0388 - accuracy: 0.9983 - val_loss: 0.0387 - val_accuracy: 0.9969\n",
            "Epoch 159/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0387 - accuracy: 0.9978 - val_loss: 0.0385 - val_accuracy: 0.9977\n",
            "Epoch 160/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0386 - accuracy: 0.9984 - val_loss: 0.0384 - val_accuracy: 0.9978\n",
            "Epoch 161/200\n",
            "100000/100000 [==============================] - 7s 65us/step - loss: 0.0385 - accuracy: 0.9982 - val_loss: 0.0382 - val_accuracy: 0.9987\n",
            "Epoch 162/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0384 - accuracy: 0.9984 - val_loss: 0.0381 - val_accuracy: 0.9995\n",
            "Epoch 163/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0384 - accuracy: 0.9983 - val_loss: 0.0381 - val_accuracy: 0.9982\n",
            "Epoch 164/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0383 - accuracy: 0.9982 - val_loss: 0.0384 - val_accuracy: 0.9962\n",
            "Epoch 165/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0383 - accuracy: 0.9985 - val_loss: 0.0380 - val_accuracy: 0.9982\n",
            "Epoch 166/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0382 - accuracy: 0.9986 - val_loss: 0.0382 - val_accuracy: 0.9964\n",
            "Epoch 167/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0381 - accuracy: 0.9979 - val_loss: 0.0379 - val_accuracy: 0.9976\n",
            "Epoch 168/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0380 - accuracy: 0.9980 - val_loss: 0.0376 - val_accuracy: 0.9995\n",
            "Epoch 169/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0379 - accuracy: 0.9988 - val_loss: 0.0379 - val_accuracy: 0.9966\n",
            "Epoch 170/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0378 - accuracy: 0.9984 - val_loss: 0.0376 - val_accuracy: 0.9979\n",
            "Epoch 171/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0378 - accuracy: 0.9982 - val_loss: 0.0374 - val_accuracy: 0.9991\n",
            "Epoch 172/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0377 - accuracy: 0.9980 - val_loss: 0.0373 - val_accuracy: 0.9995\n",
            "Epoch 173/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0376 - accuracy: 0.9985 - val_loss: 0.0375 - val_accuracy: 0.9970\n",
            "Epoch 174/200\n",
            "100000/100000 [==============================] - 7s 67us/step - loss: 0.0376 - accuracy: 0.9982 - val_loss: 0.0376 - val_accuracy: 0.9966\n",
            "Epoch 175/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0375 - accuracy: 0.9983 - val_loss: 0.0373 - val_accuracy: 0.9979\n",
            "Epoch 176/200\n",
            "100000/100000 [==============================] - 7s 69us/step - loss: 0.0374 - accuracy: 0.9981 - val_loss: 0.0372 - val_accuracy: 0.9979\n",
            "Epoch 177/200\n",
            "100000/100000 [==============================] - 7s 67us/step - loss: 0.0373 - accuracy: 0.9982 - val_loss: 0.0371 - val_accuracy: 0.9981\n",
            "Epoch 178/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0373 - accuracy: 0.9982 - val_loss: 0.0371 - val_accuracy: 0.9976\n",
            "Epoch 179/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0372 - accuracy: 0.9984 - val_loss: 0.0372 - val_accuracy: 0.9967\n",
            "Epoch 180/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0371 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9976\n",
            "Epoch 181/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0371 - accuracy: 0.9984 - val_loss: 0.0367 - val_accuracy: 0.9995\n",
            "Epoch 182/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0370 - accuracy: 0.9982 - val_loss: 0.0366 - val_accuracy: 0.9995\n",
            "Epoch 183/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0369 - accuracy: 0.9983 - val_loss: 0.0367 - val_accuracy: 0.9979\n",
            "Epoch 184/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0369 - accuracy: 0.9983 - val_loss: 0.0366 - val_accuracy: 0.9979\n",
            "Epoch 185/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0368 - accuracy: 0.9985 - val_loss: 0.0367 - val_accuracy: 0.9968\n",
            "Epoch 186/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0367 - accuracy: 0.9986 - val_loss: 0.0363 - val_accuracy: 0.9994\n",
            "Epoch 187/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0366 - accuracy: 0.9988 - val_loss: 0.0363 - val_accuracy: 0.9989\n",
            "Epoch 188/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0366 - accuracy: 0.9986 - val_loss: 0.0363 - val_accuracy: 0.9987\n",
            "Epoch 189/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0365 - accuracy: 0.9984 - val_loss: 0.0362 - val_accuracy: 0.9990\n",
            "Epoch 190/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0365 - accuracy: 0.9986 - val_loss: 0.0364 - val_accuracy: 0.9969\n",
            "Epoch 191/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0364 - accuracy: 0.9986 - val_loss: 0.0360 - val_accuracy: 0.9994\n",
            "Epoch 192/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0364 - accuracy: 0.9982 - val_loss: 0.0360 - val_accuracy: 0.9995\n",
            "Epoch 193/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0363 - accuracy: 0.9984 - val_loss: 0.0361 - val_accuracy: 0.9979\n",
            "Epoch 194/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0362 - accuracy: 0.9982 - val_loss: 0.0361 - val_accuracy: 0.9970\n",
            "Epoch 195/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0361 - accuracy: 0.9984 - val_loss: 0.0358 - val_accuracy: 0.9995\n",
            "Epoch 196/200\n",
            "100000/100000 [==============================] - 6s 63us/step - loss: 0.0361 - accuracy: 0.9982 - val_loss: 0.0358 - val_accuracy: 0.9982\n",
            "Epoch 197/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0360 - accuracy: 0.9988 - val_loss: 0.0356 - val_accuracy: 0.9990\n",
            "Epoch 198/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0359 - accuracy: 0.9986 - val_loss: 0.0356 - val_accuracy: 0.9993\n",
            "Epoch 199/200\n",
            "100000/100000 [==============================] - 6s 65us/step - loss: 0.0359 - accuracy: 0.9987 - val_loss: 0.0355 - val_accuracy: 0.9995\n",
            "Epoch 200/200\n",
            "100000/100000 [==============================] - 6s 64us/step - loss: 0.0359 - accuracy: 0.9987 - val_loss: 0.0354 - val_accuracy: 0.9991\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "20000/20000 [==============================] - 1s 35us/step\n",
            "\n",
            "Accuracy of our Model on Validation Set: 99.91500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuGcsjCOMLL3",
        "colab_type": "text"
      },
      "source": [
        "![](https://media.makeameme.org/created/path-to-zero-5cc711.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1Qg7GoRODzz",
        "colab_type": "text"
      },
      "source": [
        "Woahh!! 99.915% accuracy. That's a great start. Let's check this on test set to know that out model isn't overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZazmGJiMBgy",
        "colab_type": "text"
      },
      "source": [
        "# Testing our Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8nsa1-cpr9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model():\n",
        "    # Load test set\n",
        "    _, _, _, _, X_test, Y_test = create_dataset()\n",
        "    # Load model\n",
        "    model = load_model('/content/drive/My Drive/Rahul/ML/Is_sum_greater_than_0.5/One_layer_dense_model.h5')\n",
        "    # Get weights of our model to print\n",
        "    weights = model.layers[0].get_weights()[0]\n",
        "    bias  = model.layers[0].get_weights()[1]\n",
        "    print('Weights of our Dense layer is : ')\n",
        "    print(weights)\n",
        "    print('\\nBias of our Dense layer is : ')\n",
        "    print(bias)\n",
        "    # Test our model on test set\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose = 0)\n",
        "    print('\\nAccuracy of our Model on Test Set: %.3f' % (acc*100))\n",
        "    # Get and print wrong predictions\n",
        "    prediction = model.predict_classes(X_test)\n",
        "    prediction = np.reshape(prediction, (prediction.shape[0]))\n",
        "    wrong_predictions_X = X_test[prediction != Y_test]\n",
        "    print('\\nTotal count of wrong predictions : ' + str(wrong_predictions_X.shape[0]) + '\\n')\n",
        "    print('Sum of all four values of each wrong prediction : ')\n",
        "    print(np.reshape(np.sum(wrong_predictions_X, axis = 1), (wrong_predictions_X.shape[0], 1)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKPqdHHJpr9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "8edb216b-3cf4-47e7-e236-136c227a3479"
      },
      "source": [
        "test_model()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights of our Dense layer is : \n",
            "[[123.61418 ]\n",
            " [123.8595  ]\n",
            " [123.83697 ]\n",
            " [123.493004]]\n",
            "\n",
            "Bias of our Dense layer is : \n",
            "[-61.895977]\n",
            "\n",
            "Accuracy of our Model on Test Set: 99.860\n",
            "\n",
            "Total count of wrong predictions : 14\n",
            "\n",
            "Sum of all four values of each wrong prediction : \n",
            "[[0.50032662]\n",
            " [0.50043554]\n",
            " [0.50027449]\n",
            " [0.50002692]\n",
            " [0.50013228]\n",
            " [0.500375  ]\n",
            " [0.50037033]\n",
            " [0.50024385]\n",
            " [0.50003825]\n",
            " [0.50033815]\n",
            " [0.5003856 ]\n",
            " [0.50036001]\n",
            " [0.50056155]\n",
            " [0.50019106]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZclmosOPMee",
        "colab_type": "text"
      },
      "source": [
        "Yoo!! The model is perfect. Even the weights are in correct ratio. It is just getting wrong predictions for values whose sum is very close to 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiN1GrUOPpUm",
        "colab_type": "text"
      },
      "source": [
        "# IDEAL MODEL\n",
        "Let's create a perfect/ an ideal model by assigning weights by ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNWAx7krPytm",
        "colab_type": "text"
      },
      "source": [
        "![](https://i.imgflip.com/2hs8oa.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn4AduWhCiSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ideal_model():\n",
        "    # Load model\n",
        "    model = load_model('/content/drive/My Drive/Rahul/ML/Is_sum_greater_than_0.5/One_layer_dense_model.h5')\n",
        "    # Change the layer weights\n",
        "    model.layers[0].set_weights([np.array([[1.0000000], [1.0000000], [1.0000000], [1.0000000]]), np.array([-0.5000000])]) \n",
        "    # Get and print all the weights\n",
        "    weights = model.layers[0].get_weights()[0]\n",
        "    bias  = model.layers[0].get_weights()[1]\n",
        "    print('Weights of our Dense layer is : ')\n",
        "    print(weights)\n",
        "    print('\\nBias of our Dense layer is : ')\n",
        "    print(bias)\n",
        "    # Save our model\n",
        "    model.save('/content/drive/My Drive/Rahul/ML/Is_sum_greater_than_0.5/Ideal_model.h5')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJRBvG0PCjWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "37b5f093-4fe2-47fa-a53a-ad68c2a93042"
      },
      "source": [
        "ideal_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights of our Dense layer is : \n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "\n",
            "Bias of our Dense layer is : \n",
            "[-0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MRQGyEqTdPr",
        "colab_type": "text"
      },
      "source": [
        "Let's test our so-called ideal model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x9M9oSKCjaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_ideal_model():\n",
        "    # Load test set\n",
        "    _, _, _, _, X_test, _ = create_dataset()\n",
        "    # Creating more test set whose sum is near to 0.5\n",
        "    X = np.random.rand(100000, 4)/4\n",
        "    X = X/np.reshape(np.sum(X, axis = 1)*2, (X.shape[0], 1))\n",
        "    X = X + (np.random.rand(X.shape[0], X.shape[1])-0.5)/99999\n",
        "    # Append both the sets\n",
        "    X_test = np.append(X_test, X, axis = 0)\n",
        "    # Get correct output\n",
        "    Y_test = actual_output(X_test)\n",
        "    # Load model\n",
        "    model = load_model('/content/drive/My Drive/Rahul/ML/Is_sum_greater_than_0.5/Ideal_model.h5')\n",
        "    # Test our model on test set\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose = 0)\n",
        "    print('\\nAccuracy of our Model on Test Set : %.3f' % (acc*100))\n",
        "    # Get and print wrong predictions\n",
        "    prediction = model.predict_classes(X_test)\n",
        "    prediction = np.reshape(prediction, (prediction.shape[0]))\n",
        "    wrong_predictions_X = X_test[prediction != Y_test]\n",
        "    print('\\nTotal count of wrong predictions : ' + str(wrong_predictions_X.shape[0]) + '\\n')\n",
        "    if wrong_predictions_X.shape[0] > 0:\n",
        "        print('Sum of all four values of each wrong prediction : ')\n",
        "        print(np.reshape(np.sum(wrong_predictions_X, axis = 1), (wrong_predictions_X.shape[0], 1)))\n",
        "        wrong_predictions_Y = model.predict(wrong_predictions_X)\n",
        "        print('\\nThe softmax activation output of these wrong predictions : ')\n",
        "        print(wrong_predictions_Y)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNh4NH7HVjkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0d2bc4e-4350-41ca-9f53-41e7e20f01b4"
      },
      "source": [
        "test_ideal_model()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of our Model on Test Set : 99.479\n",
            "\n",
            "Total count of wrong predictions : 573\n",
            "\n",
            "Sum of all four values of each wrong prediction : \n",
            "[[0.50000003]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000009]\n",
            " [0.50000007]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000005]\n",
            " [0.50000009]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000001]\n",
            " [0.50000001]\n",
            " [0.50000008]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000006]\n",
            " [0.50000003]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000009]\n",
            " [0.50000008]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000008]\n",
            " [0.50000003]\n",
            " [0.50000009]\n",
            " [0.50000003]\n",
            " [0.50000009]\n",
            " [0.50000002]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.5       ]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.5       ]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.50000009]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.5000001 ]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.5000001 ]\n",
            " [0.50000005]\n",
            " [0.50000004]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.5       ]\n",
            " [0.50000005]\n",
            " [0.50000005]\n",
            " [0.50000004]\n",
            " [0.5       ]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.50000005]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000006]\n",
            " [0.50000002]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000009]\n",
            " [0.50000005]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000008]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000009]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000009]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000009]\n",
            " [0.5       ]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.5       ]\n",
            " [0.50000001]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.5       ]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000003]\n",
            " [0.50000007]\n",
            " [0.50000006]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.5       ]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000006]\n",
            " [0.50000009]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000003]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.5       ]\n",
            " [0.5000001 ]\n",
            " [0.50000003]\n",
            " [0.5       ]\n",
            " [0.50000001]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000008]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.50000003]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000008]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000007]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000009]\n",
            " [0.50000003]\n",
            " [0.5       ]\n",
            " [0.50000007]\n",
            " [0.50000003]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.5       ]\n",
            " [0.50000009]\n",
            " [0.50000009]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000003]\n",
            " [0.50000012]\n",
            " [0.50000001]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000003]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000005]\n",
            " [0.50000003]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000001]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000001]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.5       ]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000007]\n",
            " [0.50000006]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000003]\n",
            " [0.50000004]\n",
            " [0.5       ]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000008]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000009]\n",
            " [0.50000008]\n",
            " [0.50000003]\n",
            " [0.50000007]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000009]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000007]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.5       ]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000009]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000009]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000002]\n",
            " [0.50000008]\n",
            " [0.5       ]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.50000008]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000009]\n",
            " [0.50000006]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.5       ]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000007]\n",
            " [0.50000005]\n",
            " [0.50000002]\n",
            " [0.50000009]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000009]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000006]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.5       ]\n",
            " [0.50000003]\n",
            " [0.50000002]\n",
            " [0.5       ]\n",
            " [0.5000001 ]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.5       ]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000003]\n",
            " [0.50000003]\n",
            " [0.50000002]\n",
            " [0.50000006]\n",
            " [0.50000004]\n",
            " [0.50000008]\n",
            " [0.50000003]\n",
            " [0.50000003]\n",
            " [0.50000003]\n",
            " [0.50000007]\n",
            " [0.50000009]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000003]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.5       ]\n",
            " [0.50000004]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000001]\n",
            " [0.50000011]\n",
            " [0.50000005]\n",
            " [0.50000003]\n",
            " [0.50000008]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000009]\n",
            " [0.50000009]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000008]\n",
            " [0.50000004]\n",
            " [0.50000003]\n",
            " [0.50000005]\n",
            " [0.5000001 ]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000008]\n",
            " [0.5       ]\n",
            " [0.50000003]\n",
            " [0.5       ]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000006]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000005]\n",
            " [0.5       ]\n",
            " [0.5       ]\n",
            " [0.50000007]\n",
            " [0.50000008]\n",
            " [0.50000009]\n",
            " [0.50000009]\n",
            " [0.50000009]\n",
            " [0.50000006]\n",
            " [0.50000005]\n",
            " [0.50000007]\n",
            " [0.50000008]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000002]\n",
            " [0.50000001]\n",
            " [0.50000006]\n",
            " [0.50000006]\n",
            " [0.50000002]\n",
            " [0.50000007]\n",
            " [0.50000004]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000001]\n",
            " [0.50000002]\n",
            " [0.50000004]\n",
            " [0.50000005]\n",
            " [0.50000005]\n",
            " [0.50000008]\n",
            " [0.50000002]\n",
            " [0.50000009]\n",
            " [0.5       ]]\n",
            "\n",
            "The softmax activation output of these wrong predictions : \n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCNc43Y4Muuf",
        "colab_type": "text"
      },
      "source": [
        "We could see that all the wrong predictions are for the sigmoid output of 0.5. This is because of the  default threshold (Y = 1 for only X > 0.5) set on predicting classes. Currently, keras deos not allow to change the threshold. One way to solve this, is to get sigmoid output i.e. model.predict() and than compare it. Well since we are building a neural network, we will go the other way on changing it inside the keras model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PnHiAssbZH1",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/400/1*hp93DT_YP2RfPs7eBDtPfw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebxUvysUOQG2",
        "colab_type": "text"
      },
      "source": [
        "# My Ideal Model\n",
        "(I guess I am not good at making new names.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTAbnH5uPSc-",
        "colab_type": "text"
      },
      "source": [
        "## Creating a sequential class having our own defined threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0hExd2kVmMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MySequential(Sequential):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MySequential, self).__init__(**kwargs)\n",
        "    # Changing the default predict_classes function\n",
        "    def predict_classes(self, x, batch_size=32, verbose=0):\n",
        "        proba = self.predict(x, batch_size=batch_size, verbose=verbose)\n",
        "        return (proba >= 0.5).astype('int32')"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KXt8Cl5QLxt",
        "colab_type": "text"
      },
      "source": [
        "Create model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IFB-s3KP4u5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def define_my_ideal_model():\n",
        "    # Create our model\n",
        "    model = MySequential()\n",
        "    model.build(input_shape = (None, 4))\n",
        "    model.add(Dense(1, activation = 'sigmoid'))\n",
        "    # Compile our model\n",
        "    opt = SGD(learning_rate = 0.01, momentum = 0.99)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    # Change the layer weights\n",
        "    model.layers[0].set_weights([np.array([[1.0000000], [1.0000000], [1.0000000], [1.0000000]]), np.array([-0.5000000])])\n",
        "    return model"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VYo137LQOuF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_my_ideal_model():\n",
        "    model = define_my_ideal_model()\n",
        "    print(model.summary())\n",
        "    # Get and print all the weights\n",
        "    weights = model.layers[0].get_weights()[0]\n",
        "    bias  = model.layers[0].get_weights()[1]\n",
        "    print('Weights of our Dense layer is : ')\n",
        "    print(weights)\n",
        "    print('\\nBias of our Dense layer is : ')\n",
        "    print(bias)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI__hDSDQO54",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "fd95c685-85da-4150-f942-3d7e78fa6b7a"
      },
      "source": [
        "check_my_ideal_model()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"mysequential_21\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_25 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 5\n",
            "Trainable params: 5\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Weights of our Dense layer is : \n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "\n",
            "Bias of our Dense layer is : \n",
            "[-0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1No1h8EkTEQX",
        "colab_type": "text"
      },
      "source": [
        "Everything seems good, now let's test our model.\n",
        "\n",
        "(If you save this and load the model, then it will not identify MySequential class. So, I won't be saving this one.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXZC0dnFQO3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_my_ideal_model():\n",
        "    # Load test set\n",
        "    _, _, _, _, X_test, _ = create_dataset()\n",
        "    # Creating more test set whose sum is near to 0.5\n",
        "    X = np.random.rand(100000, 4)/4\n",
        "    X = X/np.reshape(np.sum(X, axis = 1)*2, (X.shape[0], 1))\n",
        "    X = X + (np.random.rand(X.shape[0], X.shape[1])-0.5)/99999\n",
        "    # Append both the sets\n",
        "    X_test = np.append(X_test, X, axis = 0)\n",
        "    # Get correct output\n",
        "    Y_test = actual_output(X_test)\n",
        "    # Load model\n",
        "    model = define_my_ideal_model()\n",
        "    # Test our model on test set\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose = 0)\n",
        "    print('\\nAccuracy of our Model on Test Set : %.3f' % (acc*100))\n",
        "    # Get and print wrong predictions\n",
        "    prediction = model.predict_classes(X_test)\n",
        "    prediction = np.reshape(prediction, (prediction.shape[0]))\n",
        "    wrong_predictions_X = X_test[prediction != Y_test]\n",
        "    print('\\nTotal count of wrong predictions : ' + str(wrong_predictions_X.shape[0]) + '\\n')\n",
        "    if wrong_predictions_X.shape[0] > 0:\n",
        "        print('Sum of all four values of each wrong prediction : ')\n",
        "        print(np.reshape(np.sum(wrong_predictions_X, axis = 1), (wrong_predictions_X.shape[0], 1)))\n",
        "        wrong_predictions_Y = model.predict(wrong_predictions_X)\n",
        "        print('\\nThe softmax activation output of these wrong predictions : ')\n",
        "        print(wrong_predictions_Y)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6jwPN93QO0Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a108e9de-9fac-41b8-91ed-c93bfab50b3a"
      },
      "source": [
        "test_my_ideal_model()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of our Model on Test Set : 99.498\n",
            "\n",
            "Total count of wrong predictions : 1034\n",
            "\n",
            "Sum of all four values of each wrong prediction : \n",
            "[[0.49999984]\n",
            " [0.49999996]\n",
            " [0.49999987]\n",
            " ...\n",
            " [0.49999991]\n",
            " [0.49999989]\n",
            " [0.49999988]]\n",
            "\n",
            "The softmax activation output of these wrong predictions : \n",
            "[[0.5]\n",
            " [0.5]\n",
            " [0.5]\n",
            " ...\n",
            " [0.5]\n",
            " [0.5]\n",
            " [0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMCsP3PhYgiC",
        "colab_type": "text"
      },
      "source": [
        "Okkay!! Ummmm!\n",
        "\n",
        "![alt text](https://media.makeameme.org/created/99-accuracy-.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0NvhQd5ZVcM",
        "colab_type": "text"
      },
      "source": [
        "Yess, got it!\n",
        "\n",
        "Keras works with float32 values, that's why, it just round offs after 7th decimal. Let's changer it to float64 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZhfBo93QOxw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.set_floatx('float64')"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS65-2GJZ1c3",
        "colab_type": "text"
      },
      "source": [
        "Let's check again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYdvvDlaYUXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "eb6f9ad2-eec8-41f4-8fea-b10f38554bd9"
      },
      "source": [
        "test_my_ideal_model()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Accuracy of our Model on Test Set : 100.000\n",
            "\n",
            "Total count of wrong predictions : 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZllho3KZ4VQ",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://wompampsupport.azureedge.net/fetchimage?siteId=7575&v=2&jpgQuality=100&width=700&url=https%3A%2F%2Fi.kym-cdn.com%2Fentries%2Ficons%2Ffacebook%2F000%2F022%2F900%2F704.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQU4E_r4bFdd",
        "colab_type": "text"
      },
      "source": [
        "## That's all for this one.\n",
        "# THANK YOU!"
      ]
    }
  ]
}